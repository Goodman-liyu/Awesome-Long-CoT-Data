<h1 align="center">
üìù Awesome Long-CoT Data
</h1>
<div align="center">

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) ![Stars](https://img.shields.io/github/stars/awesome-dpo/Awesome-DPO-Papers?color=yellow&labelColor=555555) 
</div>

# Long CoT Data Generation Methods Survey

This project aims to summarize and survey various methods for generating Long CoT (Chain-of-Thought) data. Long CoT data plays a crucial role in complex task reasoning, multi-step problem-solving, and related fields. Below is an overview of the main methods, accompanied by tables for adding relevant paper links.

---

## Method Categories

### 1. **Prompt Engineering (PE) & Short CoT Composition for Long CoT**
* **2024.01** "The Impact of Reasoning Step Length on Large Language Models". [[Paper]](https://aclanthology.org/2024.findings-acl.108.pdf)
* **2025.01** "KIMI K1.5:SCALING REINFORCEMENT LEARNING WITH LLMS". [[Paper]](https://arxiv.org/pdf/2501.12599v1)
* **2025.02** "Self-rewarding correction for mathematical reasoning". [[Paper]](https://arxiv.org/abs/2502.19613)
* **2025.02** "BOLT: Bootstrap Long Chain-of-Thought in Language Models without Distillation". [[Paper]](https://arxiv.org/abs/2502.03860)


---

### 2. **Feedback & Critique LLM Regeneration**

---

### 3. **RL-Based Deepseek Approach**

---

### 4. **Knowledge Distill**
* **2024.09** "O1 Replication Journey: A Strategic Progress Report ‚Äì Part 1". [[Paper]](https://arxiv.org/pdf/2410.18982)
* **2025.01** "RedStar: Does Scaling Long-CoT Data Unlock Better Slow-Reasoning Systems?". [[Paper]](https://arxiv.org/pdf/2501.11284)

---

### 5. **Annotation**

---

## License
This project is licensed under the [MIT License](LICENSE).
