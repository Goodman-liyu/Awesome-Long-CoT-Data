<h1 align="center">
üìù Awesome Long-CoT Data
</h1>
<div align="center">

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re) ![Stars](https://img.shields.io/github/stars/awesome-dpo/Awesome-DPO-Papers?color=yellow&labelColor=555555) 
</div>

# Long CoT Data Generation Methods Survey

This project aims to summarize and survey various methods for generating Long CoT (Chain-of-Thought) data. Long CoT data plays a crucial role in complex task reasoning, multi-step problem-solving, and related fields. Below is an overview of the main methods, accompanied by tables for adding relevant paper links.

---

## Method Categories

### 1. **Prompt Engineering (PE) & Short CoT Composition for Long CoT**
Generates Long CoT data by designing prompts (Prompt Engineering) or combining multiple Short CoTs. This approach often relies on task decomposition and step-by-step reasoning.

| Method Name | Description | Paper Link |
|-------------|-------------|------------|
| PE + Short CoT | Generates Short CoT through prompts and composes them into Long CoT | [To be added] |
| Task Decomposition | Breaks down complex tasks into subtasks and generates corresponding CoTs | [To be added] |

---

### 2. **Feedback & Critique LLM Regeneration**
Improves Long CoT data quality through feedback mechanisms or critical evaluation of LLM-generated CoTs, followed by regeneration.

| Method Name | Description | Paper Link |
|-------------|-------------|------------|
| Feedback Loop | Iteratively optimizes CoT generation through feedback mechanisms | [To be added] |
| Critique LLM | Uses LLM to critically evaluate and refine generated CoTs | [To be added] |

---

### 3. **RL-Based Deepseek Approach**
Directly generates Long CoT data using Reinforcement Learning (RL) and encourages high-quality generation through reward mechanisms.

| Method Name | Description | Paper Link |
|-------------|-------------|------------|
| Deepseek | RL-based CoT generation optimized through reward mechanisms | [To be added] |

---

### 4. **Knowledge Distill**
Generates efficient Long CoT data by distilling knowledge from large models.

| Method Name | Description | Paper Link |
|-------------|-------------|------------|
| Distill O1 | Uses distillation techniques to generate Long CoT data | [O1 Journey](https://github.com/GAIR-NLP/O1-Journey#about-the-team)|

---

### 5. **Annotation**
Generates Long CoT data through manual annotation. Due to high labor costs, this method is not the focus of this project.

---

## Contribution Guidelines
If you have relevant papers or method suggestions, feel free to submit an Issue or Pull Request. Please add the paper link to the table and briefly describe the method.

---

## References
- [To be added]

---

## License
This project is licensed under the [MIT License](LICENSE).
